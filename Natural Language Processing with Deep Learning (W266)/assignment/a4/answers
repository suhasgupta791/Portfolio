# Write your short answers in this file, replacing the placeholders as appropriate.
# This assignment consists of 2 parts for a total of 31 points.
# - Machine Translation (24 points)
# - Summarization (7 points)



###################################################################
###################################################################
## Machine Translation (24 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (a): Evaluation (6 points)  |
# ------------------------------------------------------------------

# Question 1 (/1): Why is an automated evaluation important for machine translation?
# (This question is multiple choice.  Delete all but the correct answer(s)).
machine_translation_a_1:
 - Cost
 - Speed
 - Consistency

# Question 2 (/1): What is the fundamental difficulty constructing a metric for machine translation?
# (This question is multiple choice.  Delete all but the correct answer(s)).
machine_translation_a_2:
 - No computable proxy for human judgement has been found.

# Question 3 (/1): BLEU is based on what?
# (This question is multiple choice.  Delete all but the correct answer(s)).
machine_translation_a_3:
 - Precision

# Question 4 (/1): The max credit idea is required to prevent what?
# (This question is multiple choice.  Delete all but the correct answer(s)).
machine_translation_a_4:
 - Ensuring that each reference text can only provide a bounded amount of support.  High scoring candidates must therefore have support across multiple references.
 - Giving high scores to degenerate translations that blindly list common words.

# Question 5 (/1): How does BLEU score prevent the candidate [the] being scored highly?
# (This question is multiple choice.  Delete all but the correct answer(s)).
machine_translation_a_5:
 - Max Credit
 - ngrams
 - Brevity penalty

# Question 6 (/1): Why is it still important to run human evaluations?
# (This question is multiple choice.  Delete all but the correct answer(s)).
machine_translation_a_6:
 - Correlation with human judgement is reduced when comparing candidates generated by different styles of MT systems (rules, PBMT, neural)
 - Correlation may not be as strong in the high end.  One can over optimize on the BLEU score, possibly in a way that reduces its effectiveness as a proxy for human judgement

# ------------------------------------------------------------------
# | Section (b): Sequence to Sequence Learning Paper (3 points)  |
# ------------------------------------------------------------------

# Question 1 (/1): In Sequence to Sequence Learning with Neural Networks 2014, how much additional BLEU score did they achieve by reversing the input sentence?
machine_translation_b_1: 4.70

# Question 2 (/1): The authors of the paper believe reversing input to the encoder helps to...
# (This question is multiple choice.  Delete all but the correct answer(s)).
machine_translation_b_2:
 - reduce an otherwise large minimal time lag

# Question 3 (/1): What issues are there with the seq2seq model in the paper?
# (This question is multiple choice.  Delete all but the correct answer(s)).
machine_translation_b_3:
 - They are extractive, not abstractive translations
 - RNNs do not parallelize very well, making them slow to train


# ------------------------------------------------------------------
# | Section (c): Attention (11 points)  |
# ------------------------------------------------------------------

# Question 1 (/1): In the most basic attention models, ...
# (This question is multiple choice.  Delete all but the correct answer(s)).
machine_translation_c_1:
 - a query vector is sent back from the decoder to the encoder
 - the softmax calculation in the attention mechanism has a fixed number of inputs
 - the softmax outputs can be visualized similar to the alignment models from IBM Model 1

# Question 2 (/5): If the decoder query is [3, 5, 8] and the encoder states are [3, 2, -8], [1, -2, 5], [3, 6, 9], what is the softmax input?
machine_translation_c_2: [-45, 33, 111]

# Question 3 (/5): Assume, though it is not, the previous answer was [0, 1, -1] but changing nothing else, what is the vector provided to the decoder?
machine_translation_c_3: [1.67, -0.30, 2.18]


# ------------------------------------------------------------------
# | Section (d): Beam Search (2 points)  |
# ------------------------------------------------------------------

# Question 1 (/1): As the width of beam search, changes...
# (This question is multiple choice.  Delete all but the correct answer(s)).
machine_translation_d_1:
 - ... if the width goes to 1, beam search degenerates into greedy search
 - ... if the width goes to inf, beam search degenerates into exhaustive search

# Question 2 (/1): According to the Sequence to Sequence Learning with Neural Networks paper in part b, a beam size of how large provides most of the benefits?
# (This question is multiple choice.  Delete all but the correct answer(s)).
machine_translation_d_2:
 - 2


# ------------------------------------------------------------------
# | Section (e): Transformers (2 points)  |
# ------------------------------------------------------------------

# Question 1 (/1): How are Q, K, and V constructed in the Attention is All You Need Paper?
# (This question is multiple choice.  Delete all but the correct answer(s)).
machine_translation_e_1:
 - Three learned linear transforms of the representation of each word

# Question 2 (/1): Q, K, V are three vectors used in the Attention is All You Need paper.  In the simple attention model, we only had a query and hidden state values.  Which of these played the role of the key in that simpler model?
# (This question is multiple choice.  Delete all but the correct answer(s)).
machine_translation_e_2:
 - hidden state values



###################################################################
###################################################################
## Summarization (7 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (a): Overview (7 points)  |
# ------------------------------------------------------------------

# Question 1 (/1): Abstractive summaries consist of...
# (This question is multiple choice.  Delete all but the correct answer(s)).
summarization_a_1:
 - ... new language summarizing the key points from the original document

# Question 2 (/1): Extractive summaries consist of...
# (This question is multiple choice.  Delete all but the correct answer(s)).
summarization_a_2:
 - ... a subset of the text from the original document

# Question 3 (/1): Why is single document summarization hard for machine translation, especially for news?
# (This question is multiple choice.  Delete all but the correct answer(s)).
summarization_a_3:
 - simple baseline algorithm readily available with human performance

# Question 4 (/1): ROUGE is related to BLEU in that it...
# (This question is multiple choice.  Delete all but the correct answer(s)).
summarization_a_4:
 - discourages text that is not in the reference summary

# Question 5 (/3): Get To The Point takes inspiration from both abstractive and extractive techniques.  How does it choose text from the original document to extract?
# (This question is multiple choice.  Delete all but the correct answer(s)).
summarization_a_5:
 - samples directly from the attention mechanism
